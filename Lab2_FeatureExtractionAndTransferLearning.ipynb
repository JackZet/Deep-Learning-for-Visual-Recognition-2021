{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"colab":{"name":"Lab2_FeatureExtractionAndTransferLearning.ipynb","provenance":[{"file_id":"https://github.com/aivclab/dlcourse/blob/master/Lab2_FeatureExtractionAndTransferLearning.ipynb","timestamp":1629976702215}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jdTAiE71iHF3"},"source":["# Feature extraction and transfer learning with pre-trained CNNs"]},{"cell_type":"markdown","metadata":{"id":"v7tmk836r9sK"},"source":["## Task 1: Download training images"]},{"cell_type":"markdown","metadata":{"id":"OZn30wetMkxe"},"source":["Our starting point is to find some pictures of objects that we want our neural network to recognize. To download training images of cats, for instance, you go to https://images.google.com/ and you type in \"cat\" and you just scroll through until you find a goodly bunch of them (say 200-400)\n","\n","The next thing you need to do is to get a list of all the URLs there. To do that, you need to open the [developer console](https://support.airtable.com/hc/en-us/articles/232313848-How-to-open-the-developer-console#:~:text=To%20open%20the%20developer%20console%20window%20on%20Chrome%2C%20use%20the,then%20select%20%22Developer%20Tools.%22) in your browser and you paste the following into the window that appears:\n","\n","(To open the console in Google Chrome hit Ctrl-Shift-J in Windows/Linux and Cmd-Opt-J in Mac)"]},{"cell_type":"markdown","metadata":{"id":"BVgwOGC_ifUT"},"source":["```\n","urls=Array.from(document.querySelectorAll('.rg_i')).map(el => el.getAttribute('src').startsWith('https')?el.getAttribute('src'):\"\");\n","window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1gwzN5h4stYa"},"source":["This will download a text file containing all the URLs. For now, just keep the file on your local machine. Rename the file to `<classname>.csv`. So for instance, if you downloaded URLs of cat images, you want to name the file `cat.csv`"]},{"cell_type":"markdown","metadata":{"id":"P5a5ZkTdtWz6"},"source":["Repeat the above process for each of the object categories, you wish to recognize. Pick 3 or more categories to make it more interesting. Below, I picked cat, dog, and horse."]},{"cell_type":"markdown","metadata":{"id":"5a2wNWh4zOiz"},"source":["##Task 2: Mount your Google Drive, create data directory, and upload csv files\n","Save this notebook to your Google Drive by selecting \"Save\" or \"Save a copy in Drive\" in the Files menu. If you want to store data permanently, you also need to mount your Google Drive, which can be done as follows:\n","\n","**Note:** In my browser, the copy-button that displays next to the authorizatin code doesn't work properly. So I copied the code manually."]},{"cell_type":"code","metadata":{"id":"eYezthubzNRt"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJjrzCdf0WSe"},"source":["Create data directory. I prefer to use pathlab: https://docs.python.org/3/library/pathlib.html, but you can also use normal shell commands by prefixing with !"]},{"cell_type":"code","metadata":{"id":"rH00yY6d0dWi"},"source":["from pathlib import Path\n","\n","root = '/content/gdrive/My Drive/' # Don't change this\n","#root = '/content/' # Alternative solution if mounting your Drive doesn't work\n","data_dirname = 'data' # Change as you like\n","p = Path(root + data_dirname)\n","p.mkdir(exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dp_5QZmtvoYa"},"source":["Now, let's upload the csv files to Google Colab. Click the \">\" fan at the top left below the Colab logo.\n","\n","**Note**: The graphical user interface/layout of Colab has changed since I created this notebook, so the instructinos below are not 100% up-to-date. You'll figure it out :-)"]},{"cell_type":"markdown","metadata":{"id":"srGDIAenvwWB"},"source":["![alt text](https://media.geeksforgeeks.org/wp-content/uploads/20190430123759/Screenshot-502.png)\n","\n","This will open a menu where you can see file hierachy if you click Files.\n","\n","![alt text](https://media.geeksforgeeks.org/wp-content/uploads/20190430124049/Screenshot-516.png)"]},{"cell_type":"markdown","metadata":{"id":"EXrEwFSPPb66"},"source":["To upload the csv files, locate the newly created data directory in the file hierarchy (if you used my code directly the path should be `/content/gdrive/My Drive/data`), then drag-and-drop the csv files from your local machine."]},{"cell_type":"markdown","metadata":{"id":"wdKCAv93xVeL"},"source":["It should result in something like this:"]},{"cell_type":"code","metadata":{"id":"iYlnhSOPzuWw","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"b7a60748-e255-438c-af1e-5687365bb469"},"source":["[print(x) for x in p.iterdir() if x.is_file()]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/data/cat.csv\n","/content/gdrive/My Drive/data/dog.csv\n","/content/gdrive/My Drive/data/horse.csv\n","/content/gdrive/My Drive/data/fish.csv\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[None, None, None, None]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"xegPBnK3t6ey"},"source":["If you use shell commands, you can verify that the csv files have been uploaded by listing the content of the data directory:"]},{"cell_type":"code","metadata":{"id":"V0q4lWJXugad","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"119aa2b2-3ccb-4c3b-da22-fee74b5023c8"},"source":["!ls '/content/gdrive/My Drive/data'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cat.csv  dog.csv  fish.csv  horse.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pLjprFaHvQkq"},"source":["Note that by default the current directory is /content. You can verify this by running the following command:"]},{"cell_type":"code","metadata":{"id":"F_ZVh9B6yzAC","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"08078a31-b323-4557-c9b0-cd9226378d35"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uc3DnzQ6vbG6"},"source":["##Task 3: Download image files from URLs"]},{"cell_type":"markdown","metadata":{"id":"STHrHXU1zyII"},"source":["The next step is to actually download the image files from the URLs in the csv files.\n","\n","First specify which classes you want to include. Note that the class names must match the names given to the csv files:\n","\n","**Note**: Class names must appear in alpha-numeric order to be compatible with the class assignment of the image generator below."]},{"cell_type":"code","metadata":{"id":"mStklC0j0rHf"},"source":["classes = ['cat','dog','horse']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RSwI-Rv036P"},"source":["Then download the images (just ignore any error messages):"]},{"cell_type":"code","metadata":{"id":"lZ8co4D3Pjnw"},"source":["from fastai.vision.data import download_images\n","from fastai.vision.data import verify_images\n","max_pics = 400\n","\n","for idx, name in enumerate(classes):\n","  print(name)\n","  folder = name\n","  file = name + '.csv'\n","  dest = p/folder\n","  dest.mkdir(parents=True, exist_ok=True)\n","  download_images(p/file, dest, max_pics=max_pics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G2aeQ7Oy1K_X"},"source":["And remove files that are not actually images (again, just ignore any error messages):"]},{"cell_type":"code","metadata":{"id":"rVCyQ8u-PDUL"},"source":["for c in classes:\n","    print(c)\n","    verify_images(p/c, delete=True, max_size=500)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1ZGqyqR2HS2"},"source":["## Task 4: Set up neural network for feature extraction\n","Before proceeding *REMEMBER TO ENABLE GPU IN THE RUNTIME ENVIRONMENT:* Go to Runtime -> \"Change runtime type\" and select GPU as hardware acelerator.\n","\n","We will be using a deep learning framework, called [Keras](https://keras.io/). Keras is a high-level neural network API, written in Python and capable of running on top of [TensorFlow](https://www.tensorflow.org/), CNTK, and Theano.\n","\n","A common and highly effective approach to deep learning on small image datasets is to leverage a pre-trained network. A pre-trained network is simply a saved network previously trained on a large dataset, such as the [ImageNet dataset](http://www.image-net.org/) (1.4 million labeled images and 1000 different classes). If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task.\n","\n","In our case, we will consider a convolutional neural network (CNN) trained on ImageNet. We will use the MobileNet architecture, but there are other models that you could use as well. Take a look here: https://keras.io/applications\n","\n","There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. We will be covering both of them today. Let's start with feature extraction.\n","\n","Feature extraction consists of using the representations learned by an existing neural network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch. This could be any classifier, such as K-Nearest Neighbours (K-NN).\n","\n","Traditional CNNs are divided into two parts: they start with a series of convolution and pooling layers, and they end with a densely-connected classifier. The first part is often referred to as the \"encoder\", \"feature extractor\" or \"convolutional base\" of the model. In the case of CNNs, \"feature extraction\" will simply consist of taking the convolutional base of a previously-trained network, running the new data through it, and training a new classifier on top of the output. The second part of the network, called the \"decoder\" or \"top layers\", is ignored for now. We will be using it for Transfer Learning in Task 11.\n","\n","First, let's download and instantiate the pre-trained MobileNet without the top layers (i.e., without the decoder):"]},{"cell_type":"markdown","metadata":{"id":"B0fPGZNkIG53"},"source":["###Caution\n","For some reason, you may sometimes need to downgrade tensorflow to use keras. To do this, run the following command:"]},{"cell_type":"code","metadata":{"id":"59RN53GrIcq8"},"source":["#!pip install --upgrade tensorflow==1.8.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxeBWYX76iCI"},"source":["from tensorflow.keras.applications.mobilenet import MobileNet\n","\n","conv_base = MobileNet(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(120, 120, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pn92MTLj62fD"},"source":["Let's summarize the model used for feature extraction:"]},{"cell_type":"code","metadata":{"id":"KxrNI8bk69Vj"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xm_z3RAZ7DBI"},"source":["###Questions 4.1\n","1. What is the expected shape of the input image?\n","2. What is the shape of the output of the model?\n","3. What happens to the output shape if you double the size of the input image?\n","4. Can you guess what the None dimension is used for?"]},{"cell_type":"markdown","metadata":{"id":"xSNV10j_8AFc"},"source":["##Task 5: Extract features from an image\n","Many neural networks expect the input image to have a fixed, pre-defined shape. Also, the pixel intensities are assumed to be in a fixed range. So for instance, if you train a network on images with intensities in the range -127.5 to 127.5, and you then feed the same network images with intensities in the range -0.5 to 0.5, the output of the network will most likely be garbage.\n","\n","Fortunately each pre-trained network in Keras comes with its own *preprocessor*, which assures that the intensities are scaled correctly for that particular network.\n","\n","Let's load an image, preprocess it, and feed it through the network:"]},{"cell_type":"code","metadata":{"id":"A79c9Ozj8yCV","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c5c06968-d129-499f-ffad-7ea2b8fdbdea"},"source":["import numpy as np\n","from keras.preprocessing import image\n","from keras.applications.mobilenet import preprocess_input\n","\n","# Pick first image of first class\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = filelist[1]\n","print(f\"File path: {img_path}\")\n","\n","# Load image and preprocess it\n","img = image.load_img(img_path, target_size=(120, 120))\n","img_data = image.img_to_array(img)\n","img_data = np.expand_dims(img_data, axis=0)\n","img_preprocessed = preprocess_input(img_data.copy())\n","\n","# Feed preprocessed image through CNN encoder to get a new feature representation\n","mobilenet_features = conv_base.predict(img_preprocessed)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["File path: /content/gdrive/My Drive/data/cat/00000001.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SZR30bOf-MIS"},"source":["###Questions 5.1\n","1. What is the range of the pixel values before and after preprocessing?\n","2. So what formula do you think is used to pre-process the pixel values?\n","3. What is the order of the color channels? (you could compare with openCV)?\n","4.a) What is the size of the input image?\n","4.b) What is the size of the calculated feature representation (`mobilenet_features`)?\n","4.c) So what is the reduction in dimensionality after feature extraction?"]},{"cell_type":"markdown","metadata":{"id":"Uk05mlKTr2U5"},"source":["###Extra - exploring the model further\n","We can explore the model even further. Let's look at the convolution layers:"]},{"cell_type":"code","metadata":{"id":"tPy-PaC2sMzj"},"source":["for i, layer in enumerate(conv_base.layers):\n","  \n","  # check for convolutional layer\n","  layer_type = layer.__class__.__name__\n","  \n","  if 'Conv' not in layer_type:\n","    continue\n","  \n","  # get filter weights\n","  layer_name = layer.name\n","  input_shape = layer.input_shape\n","  output_shape = layer.output.shape\n","  filter_shape = layer.get_weights()[0].shape\n","  \n","  print(f\"Layer {i} has name {layer_name}, input shape {input_shape}, filter shape {filter_shape}, and output shape {output_shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffiAfrrQuquI"},"source":["Display some filters from layer 2:"]},{"cell_type":"code","metadata":{"id":"oQrKrbbSui8G"},"source":["import matplotlib.pyplot as plt\n","layer = conv_base.layers[1] # Pick layer 1 from the list above\n","filters = layer.get_weights()\n","plt.figure(figsize=(12,12))\n","for i in range(32):\n","  f = filters[0][:,:,:,i]\n","  \n","  # Normalize to range 0 ... 1\n","  f -= f.min()\n","  f /= f.max()\n","  \n","  plt.subplot(4,8,i+1)\n","  plt.imshow(f)\n","  plt.title(str(i))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PKcT74j7veNL"},"source":["And display the resulting outputs (called **feature maps**):"]},{"cell_type":"code","metadata":{"id":"DeGt4WcVvjz_"},"source":["from keras import Model\n","dummy_model = Model(inputs=conv_base.input, outputs=conv_base.get_layer('conv1').output) # See conv_base.summary() for complete list of layer names\n","out = dummy_model.predict(img_preprocessed)\n","out = np.reshape(out,(60,60,32))\n","\n","plt.figure(figsize=(16,16))\n","for i in range(32):\n","  f = out[:,:,i]\n","  plt.subplot(4,8,i+1)\n","  plt.imshow(f,cmap='gray')\n","  plt.title(\"{0:.2f}\".format(f.min()) + \"/\" + \"{0:.2f}\".format(f.max()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsbBXOz1Uz1h"},"source":["Layers you could also inspect (layer names are written in **bold**):\n","\n","**conv1_pad** (ZeroPadding2D)    (None, 121, 121, 3)       0         \n","_________________________________________________________________\n","**conv1** (Conv2D)               (None, 60, 60, 32)        864       \n","_________________________________________________________________\n","**conv1_bn** (BatchNormalization (None, 60, 60, 32)        128       \n","_________________________________________________________________\n","**conv1_relu** (ReLU)            (None, 60, 60, 32)        0         \n","_________________________________________________________________\n","**conv_dw_1** (DepthwiseConv2D)  (None, 60, 60, 32)        288       \n","_________________________________________________________________\n","**conv_dw_1_bn** (BatchNormaliza (None, 60, 60, 32)        128       \n","_________________________________________________________________\n","**conv_dw_1_relu** (ReLU)        (None, 60, 60, 32)        0         \n","_________________________________________________________________\n","**conv_pw_1** (Conv2D)           (None, 60, 60, 64)        2048      \n","_________________________________________________________________\n","**conv_pw_1_bn** (BatchNormaliza (None, 60, 60, 64)        256       \n","_________________________________________________________________\n","**conv_pw_1_relu**(ReLU)        (None, 60, 60, 64)        0         \n","_________________________________________________________________\n","**conv_pad_2** (ZeroPadding2D)   (None, 61, 61, 64)        0         \n","\n","You may find more knowledge about the layer types here:\n","- https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\n","- https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n","- https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"]},{"cell_type":"markdown","metadata":{"id":"EIjrfD4ecsOS"},"source":["##Task 6: How to use the image generator \n","Loading and preprocessing images is such a common thing in deep learning that frameworks like Keras provide predefined tools for us that we can use. \n","In this task we will look at Keras' image data generator: https://keras.io/preprocessing/image/#imagedatagenerator-class. Simply put, the image generator is a tool that makes loading and preprocessing data easy.\n","\n","Let's set up an image generator that outputs mini-batches of 32 images:\n"]},{"cell_type":"code","metadata":{"id":"CP9tCD3Je5WP"},"source":["from keras.preprocessing.image import ImageDataGenerator\n","datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n","generator = datagen.flow_from_directory(str(p), # this is where you specify the path to the main data folder\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=32,\n","                                        class_mode='categorical',\n","                                        shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sya17-HqWD1z"},"source":["**Note:** Check that the classes assigned by the generator are consistent with your class assignment:"]},{"cell_type":"code","metadata":{"id":"qpd9vZt6WNrs","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6c4424cc-7868-4373-e2d1-2e4bfe2d2e6b"},"source":["print(generator.class_indices)\n","print(classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'cat': 0, 'dog': 1, 'horse': 2}\n","['cat', 'dog', 'horse']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5g4FSIy_fg5Q"},"source":["Here is one way to generate a new batch:"]},{"cell_type":"code","metadata":{"id":"YC9X6GshfLYv"},"source":["inputs, labels = generator.next()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P535xuM7e9jL"},"source":["###Questions 6.1\n","1. What is variable \"inputs\"?\n","2. What is variable \"labels\"?\n","3. How does the image generator know where the images are stored?\n","4. How does the image generator know the class of each image?\n","5. What does shuffle mean?"]},{"cell_type":"markdown","metadata":{"id":"6_hQXxhFgLaJ"},"source":["As you learned in lecture 2 it is always a good idea to split the data into a training set and a validation set. Again, this is such a common thing in deep learning that the image generator can do it for us:"]},{"cell_type":"code","metadata":{"id":"uXA-IppWgYsi"},"source":["datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n","\n","train_generator = datagen.flow_from_directory(str(p),\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=32,\n","                                        class_mode='categorical',\n","                                        shuffle=True,\n","                                        subset='training')\n","validation_generator = datagen.flow_from_directory(str(p),\n","                                        target_size=(120,120),\n","                                        color_mode='rgb',\n","                                        batch_size=32,\n","                                        class_mode='categorical',\n","                                        shuffle=True,\n","                                        subset='validation')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLpdkt3ehCI0"},"source":["###Questions 6.2\n","1. How does each of the two generators know if it should produce training or validation images?\n","2. What is the validation percentage in this example?"]},{"cell_type":"markdown","metadata":{"id":"5yfcUFNAhSBS"},"source":["Now, you can in principle create a complete training set and a validation set of images:"]},{"cell_type":"code","metadata":{"id":"5S9FZc68heuV"},"source":["def extract_features(generator,batch_size,num_batches):\n","    sample_count = batch_size * num_batches\n","    features = np.zeros(shape=(sample_count, 120*120*3))\n","    labels = np.zeros(shape=(sample_count))\n","    i = 0\n","    for inputs_batch, labels_batch in generator:\n","        print(i)\n","        # Use np.reshape to convert images into vectors\n","        features_batch = np.reshape(inputs_batch,(32,120*120*3))\n","        features[i * batch_size : (i + 1) * batch_size] = features_batch\n","        labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch,axis=1)\n","        i += 1\n","        if i * batch_size >= sample_count:\n","            # Note that since generators yield data indefinitely in a loop,\n","            # we must `break` after every image has been seen once.\n","            break\n","    return features, labels\n","\n","num_train_batches = 20\n","num_validation_batches = 4 \n","train_features_raw, train_labels_raw = extract_features(train_generator, 32, num_train_batches)\n","validation_features_raw, validation_labels_raw = extract_features(validation_generator, 32, num_validation_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEx-I6B20U0P"},"source":["Show the first 32 images of the validation data set (verify that the labels are correct):"]},{"cell_type":"code","metadata":{"id":"plz9liatrfhW"},"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(9,9))\n","for i in range(32):\n","  img = (np.reshape(validation_features_raw[i,:],(120,120,3))+1)/2\n","  plt.subplot(4,8,i+1)\n","  plt.imshow(img)\n","  plt.xticks([]), plt.yticks([])\n","  plt.title(classes[int(validation_labels_raw[i])])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r64k-mEESwzV"},"source":["###Questions 6.3\n","The image data has now been vectorized.\n","1. What is the shape of ```train_features_raw``` and ```train_labels_raw```?\n","2. What is the shape of ```validation_features_raw``` and ```validation_labels_raw```?\n","3. Notice the values of ```num_train_batches``` and ```num_validation_batches```. What are the implications of this?"]},{"cell_type":"markdown","metadata":{"id":"nsyddx2zjdlL"},"source":["##Task 7: Classify images using K-NN and raw pixels\n","The features that we have just calculated correspond to the raw pixel values. Now, your task is to train a K-NN classifier on the training set, and evaluate the performace on the validation set (i.e., what is the accuracy on the validation set?)\n","\n","The training set consists of variables\n","\n","```\n","train_features_raw, train_labels_raw\n","```\n","\n","and the validation set consists of variables\n","\n","```\n","validation_features_raw, validation_labels_raw\n","```\n","\n","\n","You are on your own here. You don't have to implement K-NN yourself. I suggest you use [scikit-learn](https://scikit-learn.org). Personally, I found this tutorial quite useful: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"]},{"cell_type":"markdown","metadata":{"id":"tHirJTl8esM1"},"source":["##Task 8: Classify images using K-NN and neural net features\n","Now, repeat the same task, but this time using the features calculated using the pre-trained neural network, rather than the raw pixel values.\n","\n","(*NOTE: Running this code block sometimes courses an error, when I run it. I haven't found a way to fix it yet. If it happens to you, just keep running the code block until it doesn't fail...*)"]},{"cell_type":"code","metadata":{"id":"k4PJWiZ3mAVB"},"source":["def extract_features(generator,batch_size,num_batches):\n","    sample_count = batch_size * num_batches\n","    features = np.zeros(shape=(sample_count, 3*3*1024))\n","    labels = np.zeros(shape=(sample_count))\n","    i = 0\n","    for inputs_batch, labels_batch in generator:\n","        print(i)\n","        # This is where we apply the CNN encoder to \"convert\" the image into a feature vector\n","        features_batch = conv_base.predict(inputs_batch)\n","        features_batch = np.reshape(features_batch,(32,3*3*1024))\n","        features[i * batch_size : (i + 1) * batch_size] = features_batch\n","        labels[i * batch_size : (i + 1) * batch_size] = np.argmax(labels_batch,axis=1)\n","        i += 1\n","        if i * batch_size >= sample_count:\n","            # Note that since generators yield data indefinitely in a loop,\n","            # we must `break` after every image has been seen once.\n","            break\n","    return features, labels\n","\n","num_train_batches = 20\n","num_validation_batches = 4\n","train_features, train_labels = extract_features(train_generator, 32, num_train_batches)\n","validation_features, validation_labels = extract_features(validation_generator, 32, num_validation_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN39UXf8syYV"},"source":["If everything went as planned, you should be able to conclude that the accuracy of the K-NN classifier is significantly higher when using the neural networks features compared to when using the raw pixel values as features. This is becuase the MobileNet has already been pre-trained, i.e., it has learned features that are useful for classifying 1000 different object categories. In almost any scenario you can think of, MobileNet's feature representation will be better than using the raw pixels."]},{"cell_type":"markdown","metadata":{"id":"xgy3bOwKwFrA"},"source":["##Task 9: K-means clustering\n","So what we have learned so far is that images of the same class tend to group closer together when using MobileNet's feature representation, but not so much when using the raw intensities. This confirms that using the raw pixels as features is in general a bad idea.\n","\n","The reason that MobileNet's feature representation works better is because the network has learned to map images onto a manifold. A manifold is kind of like a low-dimensional surface that exists in a high-dimensional space. For instance if images of faces were to be mapped into a 4D manifold, the first axis on the manifold could represent gender, and the others could represent age, view angle, and eye color. You can read more about manifold learning in chapter 5.11.3 of [the book](https://github.com/janishar/mit-deep-learning-book-pdf).\n","\n","The underlying hypothesis of using K-NN to classifiy images based on the features computed by MobileNet is that *objects that are similar will map to the approximate same location on some manifold.* Here we will perform K-means clustering and verify that this is in fact the case. For the record, recall that the K-means method is an *undersupervised learning method*, so it doesn't know anything about the class labels.\n","\n","Your task is to perform K-means clustering twice on your dataset: first using the raw intensity features (```train_features_raw```), then using the MobileNet features (```train_features```). Use as many clusters as you have classes.\n","\n","Again, you don't have to implement K-menas clsutering from scratch. You can use scikit-learn. I used this tutorial for inspiration: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n","\n","For each cluster, print the class labels of all images in that cluster. Explain what you observe and compare between MobileNet features and raw pixel intensities."]},{"cell_type":"markdown","metadata":{"id":"GwL76U482A9A"},"source":["##Task 10: Image retrieval\n","You can use the features of MobileNet to implement an image search engine. This is also called image retrieval.\n","\n","To make this work you need to create a new database, where each entry contains an image **and** its feature vector (as computed by MobileNet).\n","\n","The search engine work like this:\n","\n","1. Given an input image, pre-process it and feed it through MobileNet to calculate the feature vector.\n","2. Then perform a K-NN search with K=10 against the feature vectors in the database.\n","3. Then return the corresponding 10 closest images (also stored in the database)."]},{"cell_type":"markdown","metadata":{"id":"oYeEks0j_Tky"},"source":["##Task 11: Transfer learning\n"]},{"cell_type":"markdown","metadata":{"id":"UfOBC8RNXByP"},"source":["Putting your own K-NN classifier on top of a pre-trained CNN is not really optimal. Why? Because, while the features of the convolutional base are better than using raw pixel values, they are not guaranteed to 100% optimal for your specific task. So, a better solution is to attach a second neural network on top of the convolutional base, and train both the classifier *and* the convolutional base at the same time. This is called **transfer learning**. The extra neural network that put on top of our encoder is often referred to as a \"decoder\".\n","\n","Recall that CNNs like AlexNet and MobileNet have been trained on ImageNet, which contains 1000 classes. If you download Keras' pre-trained models *including the top layers* (i.e., the decoder), the top layers are in fact the classifier that we want to replace. Let's verify this:"]},{"cell_type":"code","metadata":{"id":"Kj1lyzlHXND0"},"source":["mobilenet_full = MobileNet(weights='imagenet',\n","                      include_top=True,\n","                      input_shape=(224, 224, 3))\n","mobilenet_full.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrTizMfWXmyX"},"source":["###Questions 11.1\n","Inspect the printout above.\n","1.  Can you identify the convolutional base of this network? (Compare to the ```conv_base``` model we used earlier.)\n","2. All layers beyond the convolutional base represent the classifier (or decoder). How many classes are there?\n","3. So what is the size of the output of the model?\n","4. Can you guess how we should interpret the output of model?\n","5. The input size must be 224 by 224 pixels (you can verify for yourself). Why do you think that is? "]},{"cell_type":"markdown","metadata":{"id":"iRtmTVrn2U8v"},"source":["So, how do we modify and re-train MobileNet to work on your own data? First of all, we don't want to train CNNs from scratch, since this could take days. Secondly, we need to modify the network architecture to output, say, three class labels instead of 1000.\n","\n","The main hypothesis underlying transfer learning is that the network weights learned in the convolutional layers (i.e., the *encoder*) are generic and need little or no fine-tuning to work on other data sets or tasks. So in practice, we just need to replace and re-train the last layers (i.e., the *decoder*) of a pre-trained network.\n","\n","So let's take our convolutional base (encoder) and put a simple neural network classifier (decoder) on top of it. Your task is to figure out what the value of variable N should be."]},{"cell_type":"code","metadata":{"id":"uAtJ3XgK5tTG"},"source":["N = ???"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1OMmP__03MB"},"source":["from keras.layers import Dense,GlobalAveragePooling2D\n","from keras.models import Model\n","\n","# Add new top layer\n","x = conv_base.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024,activation='relu')(x) #dense layer\n","preds = Dense(N,activation='softmax')(x) #final layer with softmax activation\n","\n","# Specify model\n","model = Model(inputs=conv_base.input, outputs=preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7jBi1s-KbE2"},"source":["Note that the weights of the new dense layers are initialized with random values.\n","\n","###Questions 11.2\n","1. What should N be in the above code block?\n","2. Re-run the code block with the correct N.\n","3. What does GlobalAveragePooling2D do?\n","\n","Hint: You can print all layers and print properties like name, type and input shape:"]},{"cell_type":"markdown","metadata":{"id":"ZO_YKGvp20I2"},"source":["We will only be training the new dense layers that we added. Disable training for all previous layers and enable for new layers:"]},{"cell_type":"code","metadata":{"id":"FL_pekjgUGnz","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"97066671-46fd-4e20-cb2c-fe14ac7bafdc"},"source":["total_num_layers = len(model.layers)\n","num_base_layers = len(conv_base.layers)\n","print(f\"Total number of layers is {total_num_layers}\")\n","print(f\"Number of pretrained base layers is {num_base_layers}\")\n","\n","for layer in model.layers[:num_base_layers]:\n","    layer.trainable=False\n","for layer in model.layers[num_base_layers:]:\n","    layer.trainable=True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of layers is 90\n","Number of pretrained base layers is 87\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1acWCuVD3xvI"},"source":["We are now ready to start training the model using\n","- Adam optimizer\n","- loss function will be categorical cross entropy\n","- evaluation metric will be accuracy\n"]},{"cell_type":"code","metadata":{"id":"_qXOMhrGM36f"},"source":["from tensorflow.keras import optimizers\n","\n","# Set up optimizer\n","sgd_optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n","\n","# Compile model - make it trainable\n","model.compile(optimizer=sgd_optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","step_size_train = train_generator.n//train_generator.batch_size # Number of mini-batches per epoch (training)\n","step_size_val = validation_generator.n//validation_generator.batch_size # Number of mini-batches per epoch (validation)\n","\n","# Train model for 10 epochs\n","history = model.fit_generator(generator=train_generator,\n","                   validation_data=validation_generator,\n","                   validation_steps=step_size_val,\n","                   steps_per_epoch=step_size_train,\n","                   epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mdAelWXnO_tG"},"source":["###Questions 11.3\n","Look at the outputs of the training.\n","\n","1. What is the difference between 'loss' and 'val_loss'?\n","2. What is the difference between 'accuracy' and 'val_accuracy'?\n","3. Do they behave the same, or do they behave differently? Try to explain what you see."]},{"cell_type":"markdown","metadata":{"id":"TtSiBK3DURiC"},"source":["##Ideas for further work\n","1. In the above example we have not optimized the pre-trained weights of the convolutional base (i.e., the encoder). To improve performance further you could enable training in all layers (including the convolutoinal base) and re-train the network. This is called *fine-tuning*.\n","2. Another way to improve model performance is by *data augmentation*. Have a look at the documentation for the [image generator class](https://keras.io/preprocessing/image/) and see what kind of augmentation is possible. Why do you think data augmentation helps improve the performance of your model?\n","3. Try repeating the above experiments on the MNIST dataset. Take a look here: https://keras.io/examples/mnist_cnn/"]},{"cell_type":"markdown","metadata":{"id":"CKlsip2RBHMn"},"source":["###Hints:\n","1. To enable training of all layers:"]},{"cell_type":"code","metadata":{"id":"BN3vaofWBCBK"},"source":["for layer in model.layers[:num_base_layers]:\n","    layer.trainable=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDW_BWllA7RM"},"source":["2. See examples of data augmentation here:\n","\n","https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/"]},{"cell_type":"code","metadata":{"id":"CVbp_WCqBs_8"},"source":["import cv2\n","\n","# Pick first image of first class\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = str(filelist[1])\n","print(f\"File path: {img_path}\")\n","\n","# Pick first image of first class\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = filelist[1]\n","print(f\"File path: {img_path}\")\n","\n","img = image.load_img(img_path, target_size=(120, 120))\n","img_data = image.img_to_array(img)\n","img_data = np.expand_dims(img_data, axis=0)\n","\n","# create image data augmentation generator\n","datagen = ImageDataGenerator(width_shift_range=[-20,20],\n","                             height_shift_range=0.5,\n","                             horizontal_flip=True,\n","                             rotation_range=30,\n","                             brightness_range=[0.2,1.0])\n","# prepare iterator\n","it = datagen.flow(img_data, batch_size=1)\n","\n","plt.figure(figsize=(9,9))\n","# generate samples and plot\n","for i in range(9):\n","\t# define subplot\n","\tplt.subplot(330 + 1 + i)\n","\t# generate batch of images\n","\tbatch = it.next()\n","\t# convert to unsigned integers for viewing\n","\timg = batch[0].astype('uint8')\n","\t# plot raw pixel data\n","\tplt.imshow(img)\n","# show the figure\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1tPX4PvTC1qe"},"source":["3. See below how to load MNIST. Your main challenge is going to be that that input images are 28x28x1, and MobileNt accepts only images that are 32x32x3 or larger. So what to do?"]},{"cell_type":"code","metadata":{"id":"hsq9pJn-C4D-"},"source":["import keras\n","from keras.datasets import mnist\n","\n","num_classes = 10\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","\n","# Convert to float32\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","# This will produce an error...\n","model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=100,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MBQU1wAPD4-w"},"source":["##Optional task: Deploying the model\n","Here is how to deploy the model and integrate with OpenCV."]},{"cell_type":"code","metadata":{"id":"LO1p9_2LWUMH"},"source":["import cv2\n","\n","# Pick first image of first class\n","filelist = [x for x in (p/classes[0]).iterdir() if x.is_file()]\n","img_path = str(filelist[1])\n","print(f\"File path: {img_path}\")\n","\n","# Load and display with OpenCV (rememebr to convert to RGB!!!)\n","img = cv2.imread(img_path)\n","b,g,r = cv2.split(img)\n","img = cv2.merge((r,g,b))\n","plt.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZMwJJ1w_YVW"},"source":["Now, make sure that the image shape and the pixel intensity range is as expected by the network:"]},{"cell_type":"code","metadata":{"id":"THYIJ5n2fX_u"},"source":["img = cv2.resize(img, (120, 120))\n","img = (img[...,::-1].astype(np.float32))\n","img /= 127.5\n","img -= 1.\n","img = np.expand_dims(img,0)\n","print(img.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52pNra-fRXRk"},"source":["Run the image through the network:"]},{"cell_type":"code","metadata":{"id":"Jeg17ghAg5H4"},"source":["pred = model.predict(img)[0]\n","ind = (-pred).argsort()[:3]\n","latex = [(classes[x],pred[x]) for x in ind]\n","print(latex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsU3tcsOJaz1"},"source":["print(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CLHhqpL_n8M"},"source":["##Optional task: Exporting to TensorFlow JS and hosting a web service on GitHub\n","If you want to, you can deploy your model and make a nice web service like this one:\n","https://klaverhenrik.github.io/transferlearning/\n","\n","To do that, first export your model to [TensorFlow JS](https://www.tensorflow.org/js) and download all the necessary files as a zip file:"]},{"cell_type":"code","metadata":{"id":"YTlAmfdLfUf-","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b37d4920-c1f2-42cb-81ca-bbe6d9ab3ba0"},"source":["!mkdir model\n","!tensorflowjs_converter --input_format keras keras.h5 model/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/bin/bash: tensorflowjs_converter: command not found\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bt2oddBTsbEr"},"source":["with open('class_names.txt', 'w') as file_handler:\n","    for item in classes:\n","        file_handler.write(\"{}\\n\".format(item))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPFHvB6qAEWP"},"source":["with open('my_classes.js','w') as file_handler:\n","  file_handler.write(\"export const IMAGENET_CLASSES = {\\n\")\n","  for ix, item in enumerate(classes):\n","    file_handler.write(\"  \" + str(ix) + \": \\'\" + item + \"\\'\")\n","    if ix < len(classes)-1:\n","      file_handler.write(\",\")\n","    file_handler.write(\"\\n\")\n","  file_handler.write(\"};\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmQ7JuCpuBpo"},"source":["!pip install tensorflowjs "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrZqydUCuDx9"},"source":["model.save('keras.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bC-as4WvuF8Y"},"source":["!mkdir model\n","!tensorflowjs_converter --input_format keras keras.h5 model/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7AcbCmLuPDs"},"source":["!cp class_names.txt model/class_names.txt\n","!cp my_classes.js model/my_classes.js"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRVc3zCGuZSg"},"source":["!zip -r model.zip model "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ij1jeyvEubZj"},"source":["from google.colab import files\n","files.download('model.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrutFOa8TQCT"},"source":["Now that you have converted you model to TensorFlow JS and downloaded, you can clone [this git repo](https://github.com/klaverhenrik/klaverhenrik.github.io/tree/master/transferlearning) and copy the model files into your own copy of the repo.\n","\n","The webpage can be hosted on GitHub using [GitHub Pages](https://pages.github.com/)."]}]}